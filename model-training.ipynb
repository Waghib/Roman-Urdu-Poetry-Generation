{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTysBPxKjxJB",
        "outputId": "fb22e707-6ac6-49c7-f44d-2d756872d876"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# ğŸ“Œ Import Necessary Libraries\n",
        "# ===============================\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import pandas as pd  # Make sure this is included\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1-ZHg8pu38E",
        "outputId": "5211cc24-e71e-4b9f-94af-8831d8cc2a67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__xXk58vHZg8",
        "outputId": "b97337a5-1689-41e4-be9d-f3df105588f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example Preprocessed Line: aankh se duur na ho dil se utar jaega \n",
            "vaqt ka kya hai guzarta hai guzar jaega \n",
            "itna manus na ho khalvat <e_token> gham se apni \n",
            "tu kabhi khud ko bhi dekhega to dar jaega \n",
            "dubte dubte kashti ko uchhala de duun \n",
            "main nahin koi to sahil pe utar jaega \n",
            "zindagi teri ata hai to ye jaane vaala \n",
            "teri bakhshish tiri dahliz pe dhar jaega \n",
            "zabt lazim hai magar dukh hai qayamat ka faraz \n",
            "zalim ab ke bhi na roega to mar jaega\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# ğŸ“Œ Load & Preprocess Data\n",
        "# ===============================\n",
        "\n",
        "# Import the required library\n",
        "import re\n",
        "\n",
        "# Load dataset (Update path as needed)\n",
        "csv_file_path = '/content/Roman-Urdu-Poetry.csv'\n",
        "df = pd.read_csv(csv_file_path)\n",
        "data = df['Poetry'].dropna().tolist()\n",
        "\n",
        "# Normalize text (convert special characters)\n",
        "def normalize_text(text):\n",
        "    # Mapping for special characters to normalize them\n",
        "    char_map = {'Ä¡': 'g', 'á¸³': 'k', 'Ã±': 'n', 'Ä': 'a', 'Ä«': 'i', 'Å«': 'u',\n",
        "                'á¹£': 's', 'á¸¥': 'h', 'á¹­': 't', 'á¸': 'd', 'á¹…': 'n', 'á¹‡': 'n', 'á¹': 'm'}\n",
        "\n",
        "    # Replace special characters based on the char_map\n",
        "    for special_char, replacement in char_map.items():\n",
        "        text = text.replace(special_char, replacement)\n",
        "\n",
        "    # Replace '-e-' with a special token '<e_token>'\n",
        "    text = text.replace('-e-', ' <e_token> ')\n",
        "\n",
        "    # Remove all punctuation (except '<e_token>')\n",
        "    text = re.sub(r'[^\\w\\s<e_token>]', '', text)\n",
        "\n",
        "    # Convert text to lowercase for consistency\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply normalization to each line of data\n",
        "data = [normalize_text(line) for line in data]\n",
        "\n",
        "# Print example data\n",
        "print(\"Example Preprocessed Line:\", data[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR64YNPXHgry",
        "outputId": "bc0455e6-9732-43a4-da71-46255c3d7caf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized Vocabulary Size: 12400\n",
            "Max Sequence Length: 100\n",
            "Shape of X (predictors): (182650, 99)\n",
            "Shape of y (labels): (182650,)\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# ğŸ“Œ Tokenization & Sequences\n",
        "# ===============================\n",
        "\n",
        "# Tokenize dataset\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(data)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create input sequences\n",
        "input_sequences = []\n",
        "for line in data:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Reduce sequence length (max 50 words)\n",
        "max_seq_len = min(100, max(len(seq) for seq in input_sequences))\n",
        "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')\n",
        "\n",
        "# Split into X (input) and y (labels)\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "\n",
        "print(\"Tokenized Vocabulary Size:\", vocab_size)\n",
        "print(\"Max Sequence Length:\", max_seq_len)\n",
        "print(\"Shape of X (predictors):\", X.shape)\n",
        "print(\"Shape of y (labels):\", y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oI2Hmk3HnCf"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# ğŸ“Œ Improved Training Setup\n",
        "# ===============================\n",
        "\n",
        "# Define Callbacks with more patience for early stopping and learning rate reduction\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, min_delta=0.001, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-5)\n",
        "\n",
        "# Modify model to have more LSTM units and layers\n",
        "model = tf.keras.Sequential([\n",
        "    Embedding(vocab_size, 128, input_length=max_seq_len - 1),  # Increase embedding size\n",
        "    LSTM(256, return_sequences=True),  # Increase LSTM units\n",
        "    Dropout(0.3),\n",
        "    LSTM(256),  # Second LSTM layer\n",
        "    Dropout(0.3),\n",
        "    Dense(256, activation='relu'),  # Additional Dense layer\n",
        "    Dropout(0.3),\n",
        "    Dense(vocab_size, activation='softmax')  # Output layer prob\n",
        "])\n",
        "\n",
        "# Define custom perplexity metric\n",
        "def perplexity(y_true, y_pred):\n",
        "    cross_entropy = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "    return tf.exp(tf.reduce_mean(cross_entropy))\n",
        "\n",
        "# Compile the model with a smaller learning rate for smoother optimization\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.0005),  # Slightly lower learning rate\n",
        "              metrics=[perplexity])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2rPa1PmHx9N",
        "outputId": "69d03711-2c9a-4953-9395-4ca3e3a4c029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 18ms/step - loss: 6.5426 - perplexity: 1107.4093 - val_loss: 6.6804 - val_perplexity: 1268.2993 - learning_rate: 5.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 19ms/step - loss: 6.1104 - perplexity: 640.9030 - val_loss: 6.7324 - val_perplexity: 1647.1924 - learning_rate: 5.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 18ms/step - loss: 5.9964 - perplexity: 538.2963 - val_loss: 6.7698 - val_perplexity: 1899.1880 - learning_rate: 5.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 18ms/step - loss: 5.9307 - perplexity: 510.6794 - val_loss: 6.7586 - val_perplexity: 2069.5815 - learning_rate: 5.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 18ms/step - loss: 5.8518 - perplexity: 482.3285 - val_loss: 6.7785 - val_perplexity: 2246.6189 - learning_rate: 5.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 18ms/step - loss: 5.7763 - perplexity: 441.0685 - val_loss: 6.7818 - val_perplexity: 2586.7993 - learning_rate: 2.5000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 18ms/step - loss: 5.7296 - perplexity: 424.9001 - val_loss: 6.7844 - val_perplexity: 2694.3953 - learning_rate: 2.5000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 18ms/step - loss: 5.6962 - perplexity: 403.9154 - val_loss: 6.7874 - val_perplexity: 2903.1575 - learning_rate: 2.5000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 18ms/step - loss: 5.6538 - perplexity: 393.5308 - val_loss: 6.8180 - val_perplexity: 3424.7190 - learning_rate: 2.5000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 18ms/step - loss: 5.6175 - perplexity: 374.7129 - val_loss: 6.8038 - val_perplexity: 3129.8047 - learning_rate: 1.2500e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 19ms/step - loss: 5.5879 - perplexity: 363.2536 - val_loss: 6.8084 - val_perplexity: 3184.7974 - learning_rate: 1.2500e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 18ms/step - loss: 5.5708 - perplexity: 355.5566 - val_loss: 6.8057 - val_perplexity: 3202.3931 - learning_rate: 1.2500e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 19ms/step - loss: 5.5582 - perplexity: 353.3126 - val_loss: 6.8074 - val_perplexity: 3196.4888 - learning_rate: 1.2500e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 18ms/step - loss: 5.5386 - perplexity: 344.0936 - val_loss: 6.8145 - val_perplexity: 3262.1497 - learning_rate: 6.2500e-05\n",
            "Epoch 15/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 18ms/step - loss: 5.5194 - perplexity: 341.5203 - val_loss: 6.8291 - val_perplexity: 3537.8452 - learning_rate: 6.2500e-05\n",
            "Epoch 16/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 18ms/step - loss: 5.5372 - perplexity: 347.0399 - val_loss: 6.8323 - val_perplexity: 3458.8625 - learning_rate: 6.2500e-05\n",
            "Epoch 17/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 18ms/step - loss: 5.4972 - perplexity: 328.2650 - val_loss: 6.8216 - val_perplexity: 3228.1147 - learning_rate: 6.2500e-05\n",
            "Epoch 18/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 19ms/step - loss: 5.5048 - perplexity: 329.7125 - val_loss: 6.8315 - val_perplexity: 3416.5715 - learning_rate: 3.1250e-05\n",
            "Epoch 19/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 18ms/step - loss: 5.4915 - perplexity: 325.2150 - val_loss: 6.8308 - val_perplexity: 3451.6558 - learning_rate: 3.1250e-05\n",
            "Epoch 20/20\n",
            "\u001b[1m9133/9133\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 18ms/step - loss: 5.4795 - perplexity: 325.6796 - val_loss: 6.8339 - val_perplexity: 3468.3745 - learning_rate: 3.1250e-05\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# ğŸ“Œ Train Model with Callbacks\n",
        "# ===============================\n",
        "\n",
        "# Train model for more epochs\n",
        "history = model.fit(X, y,\n",
        "                    epochs=20,  # Train for more epochs\n",
        "                    batch_size=16,\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=[reduce_lr])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUHeQII0H2x1",
        "outputId": "58373f98-8d9b-4173-f330-f581eaf1123b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Loss: 5.68950891494751\n",
            "Final Perplexity: 681.3063354492188\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# ğŸ“Œ Evaluate Model Perplexity\n",
        "# ===============================\n",
        "loss, ppl = model.evaluate(X, y, verbose=0)\n",
        "print(\"Final Loss:\", loss)\n",
        "print(\"Final Perplexity:\", ppl)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxotERx1S_CZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def generate_text(seed_word, next_words, max_seq_len, temperature=0.8):\n",
        "    \"\"\"\n",
        "    Generates two related verses from a single input word.\n",
        "    \"\"\"\n",
        "\n",
        "    def generate_verse(seed_text, next_words, max_seq_len, temperature):\n",
        "        \"\"\"\n",
        "        Generates a single verse using temperature-based sampling.\n",
        "        \"\"\"\n",
        "        for _ in range(next_words):\n",
        "            # Tokenize and pad sequence\n",
        "            token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "            token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=max_seq_len - 1, padding='pre')\n",
        "\n",
        "            # Predict next word probabilities\n",
        "            predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "\n",
        "            # Apply temperature-based sampling\n",
        "            predicted_probs = np.log(predicted_probs + 1e-8) / temperature\n",
        "            predicted_probs = np.exp(predicted_probs) / np.sum(np.exp(predicted_probs))\n",
        "\n",
        "            # Sample word using probability distribution\n",
        "            predicted_index = np.random.choice(range(len(predicted_probs)), p=predicted_probs)\n",
        "            output_word = tokenizer.index_word.get(predicted_index, \"\")\n",
        "\n",
        "            # Append the predicted word to seed text\n",
        "            seed_text += \" \" + output_word\n",
        "\n",
        "        return seed_text\n",
        "\n",
        "    # Generate the first verse\n",
        "    verse1 = generate_verse(seed_word, next_words, max_seq_len, temperature)\n",
        "\n",
        "    # Use the last few words of the first verse as a seed for the second verse\n",
        "    last_words = \" \".join(verse1.split()[0:])  # Taking last 3 words as seed\n",
        "    verse2 = generate_verse(last_words, next_words, max_seq_len, temperature)\n",
        "\n",
        "    return verse1, verse2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5GhRtajga5M",
        "outputId": "29b3e6e4-0c11-48c6-cc64-d458c8918e6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Verse 2: ishq liye milta nahin dillagi talak jaan bana kahte hain ki kami na kabhi hua ham se ye ek dil karna\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Example Usage\n",
        "seed_word = \"ishq\"\n",
        "verse1, verse2 = generate_text(seed_word, next_words=10, max_seq_len=15, temperature=0.8)\n",
        "\n",
        "\n",
        "print(\"Generated Verse 2:\", verse2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQFl9o6ovJ3T",
        "outputId": "0a6023b6-8ce1-4f84-c208-a22d243fa790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved successfully at /content/roman_urdu_poetry_model.keras\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# ğŸ“Œ Save Model (TensorFlow Format)\n",
        "# ===============================\n",
        "\n",
        "# Save the model using the `.keras` extension (Recommended)\n",
        "model_save_path = \"/content/roman_urdu_poetry_model.keras\"\n",
        "model.save(model_save_path)\n",
        "\n",
        "print(f\"Model saved successfully at {model_save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "uf2zM8vupcB7"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Assuming `tokenizer` is already trained\n",
        "with open(\"tokenizer.pkl\", \"wb\") as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s95XY_6iC5Fd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
